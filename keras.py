# -*- coding: utf-8 -*-
"""Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pI_X6jRJ-WD3Dzxg7D6RdCOZI8u3uMpO

# Keras Tests
This is a test of the Keras library using a convolutional neural network (CNN).  This example is based on the
[tutorial from Elite Data Science](https://elitedatascience.com/keras-tutorial-deep-learning-in-python).

We will use the MNIST dataset.  This is a collection of handwritten digits.
"""

!pip install --upgrade tensorflow==2.0
import numpy as np
import matplotlib.pyplot as plt

# Keras modules
from tensorflow.keras.models import Sequential # For constructing model
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten # Layer cores
from tensorflow.keras.layers import Conv2D, MaxPooling2D # CNN layers
from tensorflow.keras.utils import to_categorical # Extra utilities

"""## Data Formatting
First step is to load the data.
"""

from keras.datasets import mnist
(X_train,y_train), (X_test,y_test) = mnist.load_data()
print(X_train.shape)
print(X_train.shape[0])
print(y_train[0])
plt.imshow(X_train[0])

"""The data is currently loaded with dimensions (n,width,height).  The Theano backend however requires the image in the form (n,width,height,depth).

The images should also be a 32 bit float and be normalized between 0 and 1 (not sure why).
"""

X_train = X_train.reshape(X_train.shape[0],28,28,1)
X_test = X_test.reshape(X_test.shape[0],28,28,1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

"""The y data is an array of labels for each image.  For training this network, we want 10 outputs representing possible label."""

y_train = to_categorical(y_train,10)
y_test = to_categorical(y_test,10)

print(X_train.shape)
print(y_train[0])
print(len(y_train)),
print(len(X_train))

"""## Creating the Model
Now that the data is in the correct format, time to create the architecture for our model.

Described in the tutorial, our model begins with 2 convuolutional layers with 32 filters of size 3 by 3 and a relu activation function. The image is then reduced using max pooling of size 2 by 2. Next, data is randomly dropped at a rate of 25% to prevent over fitting. Finally, the image is flattened into a vector passed through a two layer neural network. This first layer as 128 nodes with a relu activation function and the second output layer has 10 nodes with a softmax activation function.
"""

model = Sequential()
 
model.add(Conv2D(32, (3,3), activation='relu', input_shape=X_train.shape[1:]))
model.add(Conv2D(32, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(rate=0.25))
 
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

"""## Train and Test
Now the model is finally ready to be trained and tested!
"""

model.fit(X_train,y_train,batch_size=32,nb_epoch=10,verbose=1)
score = model.evaluate(X_test,y_test,verbose=0)
print('Accuracy of {}'.format(score))